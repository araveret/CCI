rm(list=ls()); gc()     # clear the workspace
set.seed(973487)        # Ensures you can repeat the results
setwd("C:/Users/josdavis/Documents/Personal/GitHub/CCI")
# Get the data
data <- read.csv("titanic.csv", header = TRUE)
data <- na.omit(data)
# Split into training and testing sets
idxs <- runif(nrow(data)) < 0.8   # Random Indices
train <- data[idxs, ]             # Training set
test  <- data[!idxs, ]            # Testing set
rm(idxs, data)
library(caret)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2")
tuned_tree$bestTune
tuned_tree$results
tuned_tree
plot(tuned_tree$results$maxdepth, tuned_tree$results$Accuracy)
plot(tuned_tree)
ggplot(tuned_tree)
pred <- predict(tuned_tree, test, type = "raw")
pred
pred_proba <- predict(tuned_tree, test, type = "prob")
sum(preds == test$survived) / nrow(test)
sum(pred == test$survived) / nrow(test)
confusionMatrix(data = preds, test$survived)
confusionMatrix(data = pred, test$survived)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneLength = 5)
tuned_tree$results
plot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 4, 6, 8, 10, 12)))
tuned_tree$results
plot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 4, 6, 8, 10, 12, 50)))
tuned_tree$results
plot(tuned_tree)
tuned_tree$results
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 4, 6, 8, 10, 12, 15)))
plot(tuned_tree)
tuned_forest <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rf")
?randomForest
tuned_forest
tc <- trainControl(
# "cv" stands for cross-validation
method = "cv",
# The number of folds
number = 5)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
trControl = tc)
tuned_tree
plot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 4, 6, 8, 10, 12)),
trControl = trainControl(method = "cv", number = 5))
plot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 3, 4, 5, 6)),
trControl = trainControl(method = "cv", number = 5))
tuned_tree$results
plot(tuned_tree)
?trainControl
?train
###############################################
###                                         ###
###         PARAMETER OPTIMIZATION          ###
###                                         ###
###############################################
# ===========================================
#   Set up the workspace and get the data
# ===========================================
rm(list=ls()); gc()     # clear the workspace
set.seed(973487)        # Ensures you can repeat the results
setwd("C:/Users/josdavis/Documents/Personal/GitHub/CCI")
# Get the data
data <- read.csv("titanic.csv", header = TRUE)
data <- na.omit(data)
# Split into training and testing sets
idxs <- runif(nrow(data)) < 0.8   # Random Indices
train <- data[idxs, ]             # Training set
test  <- data[!idxs, ]            # Testing set
rm(idxs, data)
# ===========================================
#   Run and evaluate the tuning sequence
# ===========================================
library(caret)
# See here for a list of tuning parameters: http://topepo.github.io/caret/modelList.html
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2")
# Print out the best parameter option
tuned_tree$bestTune
# Print out the results for all values
tuned_tree$results
# Print out a summary of the results
tuned_tree
# Plot the results of the tuning
plot(tuned_tree$results$maxdepth, tuned_tree$results$Accuracy)
# NOTE - WHAT IS ACCURACY BASED OFF OF? (IN SAMPLE, OR OUT OF SAMPLE)
# Alternatively, caret has a built in plotting option
plot(tuned_tree)
ggplot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(2, 3, 4, 5, 6)),
trControl = trainControl(method = "cv", number = 5))
tuned_tree$results
plot(tuned_tree)
ggplot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)),
trControl = trainControl(method = "cv", number = 5))
ggplot(tuned_tree)
tuned_tree <-train(survived ~ pclass + sex + age + sibsp + parch,
data = train,
method = "rpart2",
tuneGrid = data.frame(maxdepth = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)),
trControl = trainControl(method = "cv", number = 10))
ggplot(tuned_tree)
rm(list=ls()); gc()     # clear the workspace
set.seed(973487)        # Ensures you can repeat the results
setwd("C:/Users/josdavis/Documents/Personal/GitHub/CCI")
# Get the data
data <- read.csv("titanic.csv", header = TRUE)
data <- na.omit(data)
# Split into training and testing sets
idxs <- runif(nrow(data)) < 0.8   # Random Indices
train <- data[idxs, ]             # Training set
test  <- data[!idxs, ]            # Testing set
rm(idxs, data)
?train
library(caret)
?train
?trainControl
?rm
print("hello")
?ggplot
?train
?rpart
??rpart
